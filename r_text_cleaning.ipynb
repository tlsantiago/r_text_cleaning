{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a139f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'tm' is in use and will not be installed\""
     ]
    }
   ],
   "source": [
    "install.packages(\"tm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "847acaa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'C:/Users/ssant/Desktop/Data/IGTI/Modulo01'"
      ],
      "text/latex": [
       "'C:/Users/ssant/Desktop/Data/IGTI/Modulo01'"
      ],
      "text/markdown": [
       "'C:/Users/ssant/Desktop/Data/IGTI/Modulo01'"
      ],
      "text/plain": [
       "[1] \"C:/Users/ssant/Desktop/Data/IGTI/Modulo01\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(\"tm\")\n",
    "\n",
    "getwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ffe019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in readLines(\"./textoexemplo.txt\", encoding = \"UTF-8\"):\n",
      "\"linha final incompleta encontrada em './textoexemplo.txt'\""
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'@professoraigti Dizem por aí, mas não tenho certeza, que meu sorriso fica mais feliz quando te vejo, dizem também que meus olhos brilham, dizem também que é amor, mas isso sim é certeza. Dom Casmurro. @machadodeassis #domcasmurro #machadodeassis https://www.igti.com.br/'</li>\n",
       "\t<li>'Mas a saudade é isto mesmo; é o passar e repassar das memórias antigas. Dom Casmurro.'</li>\n",
       "\t<li>'E com uma letra bem pequena, lá estava escrito no seu epitáfio: Tentou ser, não conseguiu; tentou ter, não possuiu; tentou continuar, não prosseguiu; e nessa vida de expectativas frustradas tentou até amar… Pois bem, não conseguiu, e aqui está. Dom Casmurro.'</li>\n",
       "\t<li>'O que é o Stemming?'</li>\n",
       "\t<li><span style=white-space:pre-wrap>'A linguagem natural possui diversas formas para flexionar uma palavra para que ela caiba em uma frase. É claro que essas características são determinadas de acordo com a língua que você estará utilizando.    Pense por exemplo no verbo andar:   Andei – Ande – Andarei – Andamento, Andando – Andante…'</span></li>\n",
       "\t<li>'Exemplo: Fonte: https://www.computersciencemaster.com.br/como-reduzir-uma-palavra-ao-seu-radical-em-python-stemming/ #stemming'</li>\n",
       "\t<li>'O processo de stemização (do inglês, stemming) consiste em reduzir uma palavra ao seu radical. A palavra “meninas” se reduziria a “menin”, assim como “meninos” e “menininhos”. As palavras “gato”, “gata”, “gatos” e “gatas” reduziriam-se para “gat”. A lematização reduz a palavra ao seu lema, que é a forma no masculino e singular.'</li>\n",
       "\t<li><span style=white-space:pre-wrap>'No caso de verbos, o lema é o infinitivo. Por exemplo, as palavras “gato”, “gata”, “gatos” e “gatas” são todas formas do mesmo lema: “gato”. Igualmente, as palavras “tiver”, “tenho”, “tinha”, “tem” são formas do mesmo lema “ter”. A vantagem de aplicar a stemização ou lematização é clara: redução de vocabulário e abstração de significado.   Esses pré-processamentos são de cunho morfossintático, que atuam em cima de itens lexicais, ou seja, palavras.'</span></li>\n",
       "\t<li>'O que é lematização?'</li>\n",
       "\t<li>'A lematização é o processo, efetivamente, de deflexionar uma palavra para determinar o seu lema (as flexões chamam-se lexemas). Por exemplo, as palavras gato, gata, gatos, gatas são todas formas do mesmo lema: gato. Igualmente, as palavras tiver, tenho, tinha, tem são do mesmo lema ter. E bom, melhor e ótimo são lexemas do lema bom. '</li>\n",
       "\t<li>'Exemplo: #exemplolematização Fonte: https://www.computersciencemaster.com.br/como-reduzir-uma-palavra-para-sua-forma-basica-em-python-lemmatizacao/ #lemma'</li>\n",
       "\t<li>'Lematização versus Stemming'</li>\n",
       "\t<li><span style=white-space:pre-wrap>'As duas técnicas possuem o mesmo objetivo de inflexionar uma palavra, no entanto, os outputs de uma mesma palavra são diferentes. O stemming não se importa com a legibilidade da palavra gerada como output. Já a lematização resulta em uma palavra no infinitivo (geralmente – não obrigatóriamente).   '</span></li>\n",
       "\t<li><span style=white-space:pre-wrap>'  '</span></li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '@professoraigti Dizem por aí, mas não tenho certeza, que meu sorriso fica mais feliz quando te vejo, dizem também que meus olhos brilham, dizem também que é amor, mas isso sim é certeza. Dom Casmurro. @machadodeassis \\#domcasmurro \\#machadodeassis https://www.igti.com.br/'\n",
       "\\item 'Mas a saudade é isto mesmo; é o passar e repassar das memórias antigas. Dom Casmurro.'\n",
       "\\item 'E com uma letra bem pequena, lá estava escrito no seu epitáfio: Tentou ser, não conseguiu; tentou ter, não possuiu; tentou continuar, não prosseguiu; e nessa vida de expectativas frustradas tentou até amar… Pois bem, não conseguiu, e aqui está. Dom Casmurro.'\n",
       "\\item 'O que é o Stemming?'\n",
       "\\item 'A linguagem natural possui diversas formas para flexionar uma palavra para que ela caiba em uma frase. É claro que essas características são determinadas de acordo com a língua que você estará utilizando.    Pense por exemplo no verbo andar:   Andei – Ande – Andarei – Andamento, Andando – Andante…'\n",
       "\\item 'Exemplo: Fonte: https://www.computersciencemaster.com.br/como-reduzir-uma-palavra-ao-seu-radical-em-python-stemming/ \\#stemming'\n",
       "\\item 'O processo de stemização (do inglês, stemming) consiste em reduzir uma palavra ao seu radical. A palavra “meninas” se reduziria a “menin”, assim como “meninos” e “menininhos”. As palavras “gato”, “gata”, “gatos” e “gatas” reduziriam-se para “gat”. A lematização reduz a palavra ao seu lema, que é a forma no masculino e singular.'\n",
       "\\item 'No caso de verbos, o lema é o infinitivo. Por exemplo, as palavras “gato”, “gata”, “gatos” e “gatas” são todas formas do mesmo lema: “gato”. Igualmente, as palavras “tiver”, “tenho”, “tinha”, “tem” são formas do mesmo lema “ter”. A vantagem de aplicar a stemização ou lematização é clara: redução de vocabulário e abstração de significado.   Esses pré-processamentos são de cunho morfossintático, que atuam em cima de itens lexicais, ou seja, palavras.'\n",
       "\\item 'O que é lematização?'\n",
       "\\item 'A lematização é o processo, efetivamente, de deflexionar uma palavra para determinar o seu lema (as flexões chamam-se lexemas). Por exemplo, as palavras gato, gata, gatos, gatas são todas formas do mesmo lema: gato. Igualmente, as palavras tiver, tenho, tinha, tem são do mesmo lema ter. E bom, melhor e ótimo são lexemas do lema bom. '\n",
       "\\item 'Exemplo: \\#exemplolematização Fonte: https://www.computersciencemaster.com.br/como-reduzir-uma-palavra-para-sua-forma-basica-em-python-lemmatizacao/ \\#lemma'\n",
       "\\item 'Lematização versus Stemming'\n",
       "\\item 'As duas técnicas possuem o mesmo objetivo de inflexionar uma palavra, no entanto, os outputs de uma mesma palavra são diferentes. O stemming não se importa com a legibilidade da palavra gerada como output. Já a lematização resulta em uma palavra no infinitivo (geralmente – não obrigatóriamente).   '\n",
       "\\item '  '\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '@professoraigti Dizem por aí, mas não tenho certeza, que meu sorriso fica mais feliz quando te vejo, dizem também que meus olhos brilham, dizem também que é amor, mas isso sim é certeza. Dom Casmurro. @machadodeassis #domcasmurro #machadodeassis https://www.igti.com.br/'\n",
       "2. 'Mas a saudade é isto mesmo; é o passar e repassar das memórias antigas. Dom Casmurro.'\n",
       "3. 'E com uma letra bem pequena, lá estava escrito no seu epitáfio: Tentou ser, não conseguiu; tentou ter, não possuiu; tentou continuar, não prosseguiu; e nessa vida de expectativas frustradas tentou até amar… Pois bem, não conseguiu, e aqui está. Dom Casmurro.'\n",
       "4. 'O que é o Stemming?'\n",
       "5. <span style=white-space:pre-wrap>'A linguagem natural possui diversas formas para flexionar uma palavra para que ela caiba em uma frase. É claro que essas características são determinadas de acordo com a língua que você estará utilizando.    Pense por exemplo no verbo andar:   Andei – Ande – Andarei – Andamento, Andando – Andante…'</span>\n",
       "6. 'Exemplo: Fonte: https://www.computersciencemaster.com.br/como-reduzir-uma-palavra-ao-seu-radical-em-python-stemming/ #stemming'\n",
       "7. 'O processo de stemização (do inglês, stemming) consiste em reduzir uma palavra ao seu radical. A palavra “meninas” se reduziria a “menin”, assim como “meninos” e “menininhos”. As palavras “gato”, “gata”, “gatos” e “gatas” reduziriam-se para “gat”. A lematização reduz a palavra ao seu lema, que é a forma no masculino e singular.'\n",
       "8. <span style=white-space:pre-wrap>'No caso de verbos, o lema é o infinitivo. Por exemplo, as palavras “gato”, “gata”, “gatos” e “gatas” são todas formas do mesmo lema: “gato”. Igualmente, as palavras “tiver”, “tenho”, “tinha”, “tem” são formas do mesmo lema “ter”. A vantagem de aplicar a stemização ou lematização é clara: redução de vocabulário e abstração de significado.   Esses pré-processamentos são de cunho morfossintático, que atuam em cima de itens lexicais, ou seja, palavras.'</span>\n",
       "9. 'O que é lematização?'\n",
       "10. 'A lematização é o processo, efetivamente, de deflexionar uma palavra para determinar o seu lema (as flexões chamam-se lexemas). Por exemplo, as palavras gato, gata, gatos, gatas são todas formas do mesmo lema: gato. Igualmente, as palavras tiver, tenho, tinha, tem são do mesmo lema ter. E bom, melhor e ótimo são lexemas do lema bom. '\n",
       "11. 'Exemplo: #exemplolematização Fonte: https://www.computersciencemaster.com.br/como-reduzir-uma-palavra-para-sua-forma-basica-em-python-lemmatizacao/ #lemma'\n",
       "12. 'Lematização versus Stemming'\n",
       "13. <span style=white-space:pre-wrap>'As duas técnicas possuem o mesmo objetivo de inflexionar uma palavra, no entanto, os outputs de uma mesma palavra são diferentes. O stemming não se importa com a legibilidade da palavra gerada como output. Já a lematização resulta em uma palavra no infinitivo (geralmente – não obrigatóriamente).   '</span>\n",
       "14. <span style=white-space:pre-wrap>'  '</span>\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"@professoraigti Dizem por aí, mas não tenho certeza, que meu sorriso fica mais feliz quando te vejo, dizem também que meus olhos brilham, dizem também que é amor, mas isso sim é certeza. Dom Casmurro. @machadodeassis #domcasmurro #machadodeassis https://www.igti.com.br/\"                                                                                                                                                                                      \n",
       " [2] \"Mas a saudade é isto mesmo; é o passar e repassar das memórias antigas. Dom Casmurro.\"                                                                                                                                                                                                                                                                                                                                                                               \n",
       " [3] \"E com uma letra bem pequena, lá estava escrito no seu epitáfio: Tentou ser, não conseguiu; tentou ter, não possuiu; tentou continuar, não prosseguiu; e nessa vida de expectativas frustradas tentou até amar… Pois bem, não conseguiu, e aqui está. Dom Casmurro.\"                                                                                                                                                                                                  \n",
       " [4] \"O que é o Stemming?\"                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       " [5] \"A linguagem natural possui diversas formas para flexionar uma palavra para que ela caiba em uma frase. É claro que essas características são determinadas de acordo com a língua que você estará utilizando.    Pense por exemplo no verbo andar:   Andei – Ande – Andarei – Andamento, Andando – Andante…\"                                                                                                                                                          \n",
       " [6] \"Exemplo: Fonte: https://www.computersciencemaster.com.br/como-reduzir-uma-palavra-ao-seu-radical-em-python-stemming/ #stemming\"                                                                                                                                                                                                                                                                                                                                      \n",
       " [7] \"O processo de stemização (do inglês, stemming) consiste em reduzir uma palavra ao seu radical. A palavra “meninas” se reduziria a “menin”, assim como “meninos” e “menininhos”. As palavras “gato”, “gata”, “gatos” e “gatas” reduziriam-se para “gat”. A lematização reduz a palavra ao seu lema, que é a forma no masculino e singular.\"                                                                                                                           \n",
       " [8] \"No caso de verbos, o lema é o infinitivo. Por exemplo, as palavras “gato”, “gata”, “gatos” e “gatas” são todas formas do mesmo lema: “gato”. Igualmente, as palavras “tiver”, “tenho”, “tinha”, “tem” são formas do mesmo lema “ter”. A vantagem de aplicar a stemização ou lematização é clara: redução de vocabulário e abstração de significado.   Esses pré-processamentos são de cunho morfossintático, que atuam em cima de itens lexicais, ou seja, palavras.\"\n",
       " [9] \"O que é lematização?\"                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "[10] \"A lematização é o processo, efetivamente, de deflexionar uma palavra para determinar o seu lema (as flexões chamam-se lexemas). Por exemplo, as palavras gato, gata, gatos, gatas são todas formas do mesmo lema: gato. Igualmente, as palavras tiver, tenho, tinha, tem são do mesmo lema ter. E bom, melhor e ótimo são lexemas do lema bom. \"                                                                                                                     \n",
       "[11] \"Exemplo: #exemplolematização Fonte: https://www.computersciencemaster.com.br/como-reduzir-uma-palavra-para-sua-forma-basica-em-python-lemmatizacao/ #lemma\"                                                                                                                                                                                                                                                                                                          \n",
       "[12] \"Lematização versus Stemming\"                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "[13] \"As duas técnicas possuem o mesmo objetivo de inflexionar uma palavra, no entanto, os outputs de uma mesma palavra são diferentes. O stemming não se importa com a legibilidade da palavra gerada como output. Já a lematização resulta em uma palavra no infinitivo (geralmente – não obrigatóriamente).   \"                                                                                                                                                         \n",
       "[14] \"  \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Read file\n",
    "texto <- unclean_text <- readLines(\"./textoexemplo.txt\",encoding='UTF-8')\n",
    "unclean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd788381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data as a corpus\n",
    "TextDoc <- Corpus(VectorSource(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcc2cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#toSpace creation \n",
    "toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern, \" \", x))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ed6bc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in tm_map.SimpleCorpus(TextDoc, toSpace, \"@\\\\w+\"):\n",
      "\"transformation drops documents\"Warning message in tm_map.SimpleCorpus(TextDoc, toSpace, \"#\\\\w+\"):\n",
      "\"transformation drops documents\"Warning message in tm_map.SimpleCorpus(TextDoc, toSpace, \" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\"):\n",
      "\"transformation drops documents\""
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'  Dizem por aí, mas não tenho certeza, que meu sorriso fica mais feliz quando te vejo, dizem também que meus olhos brilham, dizem também que é amor, mas isso sim é certeza. Dom Casmurro.       '</span>"
      ],
      "text/latex": [
       "'  Dizem por aí, mas não tenho certeza, que meu sorriso fica mais feliz quando te vejo, dizem também que meus olhos brilham, dizem também que é amor, mas isso sim é certeza. Dom Casmurro.       '"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'  Dizem por aí, mas não tenho certeza, que meu sorriso fica mais feliz quando te vejo, dizem também que meus olhos brilham, dizem também que é amor, mas isso sim é certeza. Dom Casmurro.       '</span>"
      ],
      "text/plain": [
       "[1] \"  Dizem por aí, mas não tenho certeza, que meu sorriso fica mais feliz quando te vejo, dizem também que meus olhos brilham, dizem também que é amor, mas isso sim é certeza. Dom Casmurro.       \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cleaning\n",
    "TextDoc <- tm_map(TextDoc, toSpace, \"@\\\\w+\")\n",
    "TextDoc <- tm_map(TextDoc, toSpace, \"#\\\\w+\")\n",
    "TextDoc <- tm_map(TextDoc, toSpace, \" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\")\n",
    "\n",
    "#Viz\n",
    "TextDoc[[1]]$content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f691c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'removeNumbers'</li>\n",
       "\t<li>'removePunctuation'</li>\n",
       "\t<li>'removeWords'</li>\n",
       "\t<li>'stemDocument'</li>\n",
       "\t<li>'stripWhitespace'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'removeNumbers'\n",
       "\\item 'removePunctuation'\n",
       "\\item 'removeWords'\n",
       "\\item 'stemDocument'\n",
       "\\item 'stripWhitespace'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'removeNumbers'\n",
       "2. 'removePunctuation'\n",
       "3. 'removeWords'\n",
       "4. 'stemDocument'\n",
       "5. 'stripWhitespace'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"removeNumbers\"     \"removePunctuation\" \"removeWords\"      \n",
       "[4] \"stemDocument\"      \"stripWhitespace\"  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "getTransformations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46756119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in tm_map.SimpleCorpus(TextDoc, removePunctuation):\n",
      "\"transformation drops documents\""
     ]
    }
   ],
   "source": [
    "#Remove punctuation – replace punctuation marks with ” “\n",
    "TextDoc <- tm_map(TextDoc, removePunctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f81f6ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in tm_map.SimpleCorpus(TextDoc, toSpace, \"–\"):\n",
      "\"transformation drops documents\"Warning message in tm_map.SimpleCorpus(TextDoc, toSpace, \"“\"):\n",
      "\"transformation drops documents\"Warning message in tm_map.SimpleCorpus(TextDoc, toSpace, \"”\"):\n",
      "\"transformation drops documents\""
     ]
    }
   ],
   "source": [
    "TextDoc <- tm_map(TextDoc, toSpace, \"–\")\n",
    "TextDoc <- tm_map(TextDoc, toSpace, \"“\")\n",
    "TextDoc <- tm_map(TextDoc, toSpace, \"”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e24526d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in tm_map.SimpleCorpus(TextDoc, content_transformer(tolower)):\n",
      "\"transformation drops documents\""
     ]
    }
   ],
   "source": [
    "#Transform to lower case (need to wrap in content_transformer)\n",
    "TextDoc <- tm_map(TextDoc,content_transformer(tolower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2172259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in tm_map.SimpleCorpus(TextDoc, removeNumbers):\n",
      "\"transformation drops documents\""
     ]
    }
   ],
   "source": [
    "#Strip digits (std transformation, so no need for content_transformer)\n",
    "TextDoc <- tm_map(TextDoc, removeNumbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8722c544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [1] \"de\"           \"a\"            \"o\"            \"que\"          \"e\"           \n",
      "  [6] \"do\"           \"da\"           \"em\"           \"um\"           \"para\"        \n",
      " [11] \"com\"          \"não\"          \"uma\"          \"os\"           \"no\"          \n",
      " [16] \"se\"           \"na\"           \"por\"          \"mais\"         \"as\"          \n",
      " [21] \"dos\"          \"como\"         \"mas\"          \"ao\"           \"ele\"         \n",
      " [26] \"das\"          \"à\"            \"seu\"          \"sua\"          \"ou\"          \n",
      " [31] \"quando\"       \"muito\"        \"nos\"          \"já\"           \"eu\"          \n",
      " [36] \"também\"       \"só\"           \"pelo\"         \"pela\"         \"até\"         \n",
      " [41] \"isso\"         \"ela\"          \"entre\"        \"depois\"       \"sem\"         \n",
      " [46] \"mesmo\"        \"aos\"          \"seus\"         \"quem\"         \"nas\"         \n",
      " [51] \"me\"           \"esse\"         \"eles\"         \"você\"         \"essa\"        \n",
      " [56] \"num\"          \"nem\"          \"suas\"         \"meu\"          \"às\"          \n",
      " [61] \"minha\"        \"numa\"         \"pelos\"        \"elas\"         \"qual\"        \n",
      " [66] \"nós\"          \"lhe\"          \"deles\"        \"essas\"        \"esses\"       \n",
      " [71] \"pelas\"        \"este\"         \"dele\"         \"tu\"           \"te\"          \n",
      " [76] \"vocês\"        \"vos\"          \"lhes\"         \"meus\"         \"minhas\"      \n",
      " [81] \"teu\"          \"tua\"          \"teus\"         \"tuas\"         \"nosso\"       \n",
      " [86] \"nossa\"        \"nossos\"       \"nossas\"       \"dela\"         \"delas\"       \n",
      " [91] \"esta\"         \"estes\"        \"estas\"        \"aquele\"       \"aquela\"      \n",
      " [96] \"aqueles\"      \"aquelas\"      \"isto\"         \"aquilo\"       \"estou\"       \n",
      "[101] \"está\"         \"estamos\"      \"estão\"        \"estive\"       \"esteve\"      \n",
      "[106] \"estivemos\"    \"estiveram\"    \"estava\"       \"estávamos\"    \"estavam\"     \n",
      "[111] \"estivera\"     \"estivéramos\"  \"esteja\"       \"estejamos\"    \"estejam\"     \n",
      "[116] \"estivesse\"    \"estivéssemos\" \"estivessem\"   \"estiver\"      \"estivermos\"  \n",
      "[121] \"estiverem\"    \"hei\"          \"há\"           \"havemos\"      \"hão\"         \n",
      "[126] \"houve\"        \"houvemos\"     \"houveram\"     \"houvera\"      \"houvéramos\"  \n",
      "[131] \"haja\"         \"hajamos\"      \"hajam\"        \"houvesse\"     \"houvéssemos\" \n",
      "[136] \"houvessem\"    \"houver\"       \"houvermos\"    \"houverem\"     \"houverei\"    \n",
      "[141] \"houverá\"      \"houveremos\"   \"houverão\"     \"houveria\"     \"houveríamos\" \n",
      "[146] \"houveriam\"    \"sou\"          \"somos\"        \"são\"          \"era\"         \n",
      "[151] \"éramos\"       \"eram\"         \"fui\"          \"foi\"          \"fomos\"       \n",
      "[156] \"foram\"        \"fora\"         \"fôramos\"      \"seja\"         \"sejamos\"     \n",
      "[161] \"sejam\"        \"fosse\"        \"fôssemos\"     \"fossem\"       \"for\"         \n",
      "[166] \"formos\"       \"forem\"        \"serei\"        \"será\"         \"seremos\"     \n",
      "[171] \"serão\"        \"seria\"        \"seríamos\"     \"seriam\"       \"tenho\"       \n",
      "[176] \"tem\"          \"temos\"        \"tém\"          \"tinha\"        \"tínhamos\"    \n",
      "[181] \"tinham\"       \"tive\"         \"teve\"         \"tivemos\"      \"tiveram\"     \n",
      "[186] \"tivera\"       \"tivéramos\"    \"tenha\"        \"tenhamos\"     \"tenham\"      \n",
      "[191] \"tivesse\"      \"tivéssemos\"   \"tivessem\"     \"tiver\"        \"tivermos\"    \n",
      "[196] \"tiverem\"      \"terei\"        \"terá\"         \"teremos\"      \"terão\"       \n",
      "[201] \"teria\"        \"teríamos\"     \"teriam\"      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in tm_map.SimpleCorpus(TextDoc, removeWords, \"é\"):\n",
      "\"transformation drops documents\""
     ]
    }
   ],
   "source": [
    "print(stopwords(\"portuguese\"))\n",
    "\n",
    "#remove stopwords using the standard list in tm\n",
    "TextDoc <- tm_map(TextDoc, removeWords, \"é\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bc3fd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in tm_map.SimpleCorpus(TextDoc, stripWhitespace):\n",
      "\"transformation drops documents\""
     ]
    }
   ],
   "source": [
    "#Strip whitespace\n",
    "TextDoc <- tm_map(TextDoc, stripWhitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6770e475",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in loadNamespace(name): there is no package called 'SnowballC'\n",
     "output_type": "error",
     "traceback": [
      "Error in loadNamespace(name): there is no package called 'SnowballC'\nTraceback:\n",
      "1. tm_map(TextDoc, stemDocument)",
      "2. tm_map.SimpleCorpus(TextDoc, stemDocument)",
      "3. FUN(content(x), ...)",
      "4. stemDocument.character(content(x), ...)",
      "5. unlist(lapply(x, function(line) paste(SnowballC::wordStem(words(line), \n .     as.character(language)), collapse = \" \")))",
      "6. lapply(x, function(line) paste(SnowballC::wordStem(words(line), \n .     as.character(language)), collapse = \" \"))",
      "7. FUN(X[[i]], ...)",
      "8. paste(SnowballC::wordStem(words(line), as.character(language)), \n .     collapse = \" \")",
      "9. SnowballC::wordStem",
      "10. getExportedValue(pkg, name)",
      "11. asNamespace(ns)",
      "12. getNamespace(ns)",
      "13. loadNamespace(name)",
      "14. withRestarts(stop(cond), retry_loadNamespace = function() NULL)",
      "15. withOneRestart(expr, restarts[[1L]])",
      "16. doWithOneRestart(return(expr), restart)"
     ]
    }
   ],
   "source": [
    "#stem the corpus\n",
    "TextDoc <- tm_map(TextDoc, stemDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "190e5fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<<TermDocumentMatrix (terms: 168, documents: 14)>>\n",
       "Non-/sparse entries: 242/2110\n",
       "Sparsity           : 90%\n",
       "Maximal term length: 17\n",
       "Weighting          : term frequency (tf)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create term-document matrix\n",
    "TextDoc_matrix <- TermDocumentMatrix(TextDoc)\n",
    "\n",
    "TextDoc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5bcef946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming into a manipulable matrix\n",
    "matriz <- as.matrix(TextDoc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff56c46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>que</dt>\n",
       "\t\t<dd>10</dd>\n",
       "\t<dt>palavra</dt>\n",
       "\t\t<dd>9</dd>\n",
       "\t<dt>uma</dt>\n",
       "\t\t<dd>8</dd>\n",
       "\t<dt>são</dt>\n",
       "\t\t<dd>8</dd>\n",
       "\t<dt>lema</dt>\n",
       "\t\t<dd>8</dd>\n",
       "\t<dt>não</dt>\n",
       "\t\t<dd>7</dd>\n",
       "\t<dt>mesmo</dt>\n",
       "\t\t<dd>6</dd>\n",
       "\t<dt>lematização</dt>\n",
       "\t\t<dd>6</dd>\n",
       "\t<dt>palavras</dt>\n",
       "\t\t<dd>6</dd>\n",
       "\t<dt>exemplo</dt>\n",
       "\t\t<dd>5</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[que] 10\n",
       "\\item[palavra] 9\n",
       "\\item[uma] 8\n",
       "\\item[são] 8\n",
       "\\item[lema] 8\n",
       "\\item[não] 7\n",
       "\\item[mesmo] 6\n",
       "\\item[lematização] 6\n",
       "\\item[palavras] 6\n",
       "\\item[exemplo] 5\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "que\n",
       ":   10palavra\n",
       ":   9uma\n",
       ":   8são\n",
       ":   8lema\n",
       ":   8não\n",
       ":   7mesmo\n",
       ":   6lematização\n",
       ":   6palavras\n",
       ":   6exemplo\n",
       ":   5\n",
       "\n"
      ],
      "text/plain": [
       "        que     palavra         uma         são        lema         não \n",
       "         10           9           8           8           8           7 \n",
       "      mesmo lematização    palavras     exemplo \n",
       "          6           6           6           5 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Descending order organization\n",
    "matriz <- sort(rowSums(matriz), decreasing=T)\n",
    "\n",
    "head(matriz,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1199f3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>word</th><th scope=col>freq</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>que</th><td>que        </td><td>10         </td></tr>\n",
       "\t<tr><th scope=row>palavra</th><td>palavra    </td><td> 9         </td></tr>\n",
       "\t<tr><th scope=row>uma</th><td>uma        </td><td> 8         </td></tr>\n",
       "\t<tr><th scope=row>são</th><td>são        </td><td> 8         </td></tr>\n",
       "\t<tr><th scope=row>lema</th><td>lema       </td><td> 8         </td></tr>\n",
       "\t<tr><th scope=row>não</th><td>não        </td><td> 7         </td></tr>\n",
       "\t<tr><th scope=row>mesmo</th><td>mesmo      </td><td> 6         </td></tr>\n",
       "\t<tr><th scope=row>lematização</th><td>lematização</td><td> 6         </td></tr>\n",
       "\t<tr><th scope=row>palavras</th><td>palavras   </td><td> 6         </td></tr>\n",
       "\t<tr><th scope=row>exemplo</th><td>exemplo    </td><td> 5         </td></tr>\n",
       "\t<tr><th scope=row>gato</th><td>gato       </td><td> 5         </td></tr>\n",
       "\t<tr><th scope=row>por</th><td>por        </td><td> 4         </td></tr>\n",
       "\t<tr><th scope=row>seu</th><td>seu        </td><td> 4         </td></tr>\n",
       "\t<tr><th scope=row>tentou</th><td>tentou     </td><td> 4         </td></tr>\n",
       "\t<tr><th scope=row>stemming</th><td>stemming   </td><td> 4         </td></tr>\n",
       "\t<tr><th scope=row>formas</th><td>formas     </td><td> 4         </td></tr>\n",
       "\t<tr><th scope=row>para</th><td>para       </td><td> 4         </td></tr>\n",
       "\t<tr><th scope=row>casmurro</th><td>casmurro   </td><td> 3         </td></tr>\n",
       "\t<tr><th scope=row>dizem</th><td>dizem      </td><td> 3         </td></tr>\n",
       "\t<tr><th scope=row>dom</th><td>dom        </td><td> 3         </td></tr>\n",
       "\t<tr><th scope=row>mas</th><td>mas        </td><td> 3         </td></tr>\n",
       "\t<tr><th scope=row>tenho</th><td>tenho      </td><td> 3         </td></tr>\n",
       "\t<tr><th scope=row>com</th><td>com        </td><td> 3         </td></tr>\n",
       "\t<tr><th scope=row>ter</th><td>ter        </td><td> 3         </td></tr>\n",
       "\t<tr><th scope=row>gata</th><td>gata       </td><td> 3         </td></tr>\n",
       "\t<tr><th scope=row>gatas</th><td>gatas      </td><td> 3         </td></tr>\n",
       "\t<tr><th scope=row>gatos</th><td>gatos      </td><td> 3         </td></tr>\n",
       "\t<tr><th scope=row>certeza</th><td>certeza    </td><td> 2         </td></tr>\n",
       "\t<tr><th scope=row>também</th><td>também     </td><td> 2         </td></tr>\n",
       "\t<tr><th scope=row>bem</th><td>bem        </td><td> 2         </td></tr>\n",
       "\t<tr><th scope=row>...</th><td>...</td><td>...</td></tr>\n",
       "\t<tr><th scope=row>seja</th><td>seja            </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>significado</th><td>significado     </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>vantagem</th><td>vantagem        </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>verbos</th><td>verbos          </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>vocabulário</th><td>vocabulário     </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>chamamse</th><td>chamamse        </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>deflexionar</th><td>deflexionar     </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>determinar</th><td>determinar      </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>efetivamente</th><td>efetivamente    </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>flexões</th><td>flexões         </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>melhor</th><td>melhor          </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>ótimo</th><td>ótimo           </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>versus</th><td>versus          </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>diferentes</th><td>diferentes      </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>duas</th><td>duas            </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>entanto</th><td>entanto         </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>gerada</th><td>gerada          </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>geralmente</th><td>geralmente      </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>importa</th><td>importa         </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>inflexionar</th><td>inflexionar     </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>já</th><td>já              </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>legibilidade</th><td>legibilidade    </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>mesma</th><td>mesma           </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>objetivo</th><td>objetivo        </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>obrigatóriamente</th><td>obrigatóriamente</td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>output</th><td>output          </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>outputs</th><td>outputs         </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>possuem</th><td>possuem         </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>resulta</th><td>resulta         </td><td>1               </td></tr>\n",
       "\t<tr><th scope=row>técnicas</th><td>técnicas        </td><td>1               </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & word & freq\\\\\n",
       "\\hline\n",
       "\tque & que         & 10         \\\\\n",
       "\tpalavra & palavra     &  9         \\\\\n",
       "\tuma & uma         &  8         \\\\\n",
       "\tsão & são         &  8         \\\\\n",
       "\tlema & lema        &  8         \\\\\n",
       "\tnão & não         &  7         \\\\\n",
       "\tmesmo & mesmo       &  6         \\\\\n",
       "\tlematização & lematização &  6         \\\\\n",
       "\tpalavras & palavras    &  6         \\\\\n",
       "\texemplo & exemplo     &  5         \\\\\n",
       "\tgato & gato        &  5         \\\\\n",
       "\tpor & por         &  4         \\\\\n",
       "\tseu & seu         &  4         \\\\\n",
       "\ttentou & tentou      &  4         \\\\\n",
       "\tstemming & stemming    &  4         \\\\\n",
       "\tformas & formas      &  4         \\\\\n",
       "\tpara & para        &  4         \\\\\n",
       "\tcasmurro & casmurro    &  3         \\\\\n",
       "\tdizem & dizem       &  3         \\\\\n",
       "\tdom & dom         &  3         \\\\\n",
       "\tmas & mas         &  3         \\\\\n",
       "\ttenho & tenho       &  3         \\\\\n",
       "\tcom & com         &  3         \\\\\n",
       "\tter & ter         &  3         \\\\\n",
       "\tgata & gata        &  3         \\\\\n",
       "\tgatas & gatas       &  3         \\\\\n",
       "\tgatos & gatos       &  3         \\\\\n",
       "\tcerteza & certeza     &  2         \\\\\n",
       "\ttambém & também      &  2         \\\\\n",
       "\tbem & bem         &  2         \\\\\n",
       "\t... & ... & ...\\\\\n",
       "\tseja & seja             & 1               \\\\\n",
       "\tsignificado & significado      & 1               \\\\\n",
       "\tvantagem & vantagem         & 1               \\\\\n",
       "\tverbos & verbos           & 1               \\\\\n",
       "\tvocabulário & vocabulário      & 1               \\\\\n",
       "\tchamamse & chamamse         & 1               \\\\\n",
       "\tdeflexionar & deflexionar      & 1               \\\\\n",
       "\tdeterminar & determinar       & 1               \\\\\n",
       "\tefetivamente & efetivamente     & 1               \\\\\n",
       "\tflexões & flexões          & 1               \\\\\n",
       "\tmelhor & melhor           & 1               \\\\\n",
       "\tótimo & ótimo            & 1               \\\\\n",
       "\tversus & versus           & 1               \\\\\n",
       "\tdiferentes & diferentes       & 1               \\\\\n",
       "\tduas & duas             & 1               \\\\\n",
       "\tentanto & entanto          & 1               \\\\\n",
       "\tgerada & gerada           & 1               \\\\\n",
       "\tgeralmente & geralmente       & 1               \\\\\n",
       "\timporta & importa          & 1               \\\\\n",
       "\tinflexionar & inflexionar      & 1               \\\\\n",
       "\tjá & já               & 1               \\\\\n",
       "\tlegibilidade & legibilidade     & 1               \\\\\n",
       "\tmesma & mesma            & 1               \\\\\n",
       "\tobjetivo & objetivo         & 1               \\\\\n",
       "\tobrigatóriamente & obrigatóriamente & 1               \\\\\n",
       "\toutput & output           & 1               \\\\\n",
       "\toutputs & outputs          & 1               \\\\\n",
       "\tpossuem & possuem          & 1               \\\\\n",
       "\tresulta & resulta          & 1               \\\\\n",
       "\ttécnicas & técnicas         & 1               \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | word | freq |\n",
       "|---|---|---|\n",
       "| que | que         | 10          |\n",
       "| palavra | palavra     |  9          |\n",
       "| uma | uma         |  8          |\n",
       "| são | são         |  8          |\n",
       "| lema | lema        |  8          |\n",
       "| não | não         |  7          |\n",
       "| mesmo | mesmo       |  6          |\n",
       "| lematização | lematização |  6          |\n",
       "| palavras | palavras    |  6          |\n",
       "| exemplo | exemplo     |  5          |\n",
       "| gato | gato        |  5          |\n",
       "| por | por         |  4          |\n",
       "| seu | seu         |  4          |\n",
       "| tentou | tentou      |  4          |\n",
       "| stemming | stemming    |  4          |\n",
       "| formas | formas      |  4          |\n",
       "| para | para        |  4          |\n",
       "| casmurro | casmurro    |  3          |\n",
       "| dizem | dizem       |  3          |\n",
       "| dom | dom         |  3          |\n",
       "| mas | mas         |  3          |\n",
       "| tenho | tenho       |  3          |\n",
       "| com | com         |  3          |\n",
       "| ter | ter         |  3          |\n",
       "| gata | gata        |  3          |\n",
       "| gatas | gatas       |  3          |\n",
       "| gatos | gatos       |  3          |\n",
       "| certeza | certeza     |  2          |\n",
       "| também | também      |  2          |\n",
       "| bem | bem         |  2          |\n",
       "| ... | ... | ... |\n",
       "| seja | seja             | 1                |\n",
       "| significado | significado      | 1                |\n",
       "| vantagem | vantagem         | 1                |\n",
       "| verbos | verbos           | 1                |\n",
       "| vocabulário | vocabulário      | 1                |\n",
       "| chamamse | chamamse         | 1                |\n",
       "| deflexionar | deflexionar      | 1                |\n",
       "| determinar | determinar       | 1                |\n",
       "| efetivamente | efetivamente     | 1                |\n",
       "| flexões | flexões          | 1                |\n",
       "| melhor | melhor           | 1                |\n",
       "| ótimo | ótimo            | 1                |\n",
       "| versus | versus           | 1                |\n",
       "| diferentes | diferentes       | 1                |\n",
       "| duas | duas             | 1                |\n",
       "| entanto | entanto          | 1                |\n",
       "| gerada | gerada           | 1                |\n",
       "| geralmente | geralmente       | 1                |\n",
       "| importa | importa          | 1                |\n",
       "| inflexionar | inflexionar      | 1                |\n",
       "| já | já               | 1                |\n",
       "| legibilidade | legibilidade     | 1                |\n",
       "| mesma | mesma            | 1                |\n",
       "| objetivo | objetivo         | 1                |\n",
       "| obrigatóriamente | obrigatóriamente | 1                |\n",
       "| output | output           | 1                |\n",
       "| outputs | outputs          | 1                |\n",
       "| possuem | possuem          | 1                |\n",
       "| resulta | resulta          | 1                |\n",
       "| técnicas | técnicas         | 1                |\n",
       "\n"
      ],
      "text/plain": [
       "                 word             freq\n",
       "que              que              10  \n",
       "palavra          palavra           9  \n",
       "uma              uma               8  \n",
       "são              são               8  \n",
       "lema             lema              8  \n",
       "não              não               7  \n",
       "mesmo            mesmo             6  \n",
       "lematização      lematização       6  \n",
       "palavras         palavras          6  \n",
       "exemplo          exemplo           5  \n",
       "gato             gato              5  \n",
       "por              por               4  \n",
       "seu              seu               4  \n",
       "tentou           tentou            4  \n",
       "stemming         stemming          4  \n",
       "formas           formas            4  \n",
       "para             para              4  \n",
       "casmurro         casmurro          3  \n",
       "dizem            dizem             3  \n",
       "dom              dom               3  \n",
       "mas              mas               3  \n",
       "tenho            tenho             3  \n",
       "com              com               3  \n",
       "ter              ter               3  \n",
       "gata             gata              3  \n",
       "gatas            gatas             3  \n",
       "gatos            gatos             3  \n",
       "certeza          certeza           2  \n",
       "também           também            2  \n",
       "bem              bem               2  \n",
       "...              ...              ... \n",
       "seja             seja             1   \n",
       "significado      significado      1   \n",
       "vantagem         vantagem         1   \n",
       "verbos           verbos           1   \n",
       "vocabulário      vocabulário      1   \n",
       "chamamse         chamamse         1   \n",
       "deflexionar      deflexionar      1   \n",
       "determinar       determinar       1   \n",
       "efetivamente     efetivamente     1   \n",
       "flexões          flexões          1   \n",
       "melhor           melhor           1   \n",
       "ótimo            ótimo            1   \n",
       "versus           versus           1   \n",
       "diferentes       diferentes       1   \n",
       "duas             duas             1   \n",
       "entanto          entanto          1   \n",
       "gerada           gerada           1   \n",
       "geralmente       geralmente       1   \n",
       "importa          importa          1   \n",
       "inflexionar      inflexionar      1   \n",
       "já               já               1   \n",
       "legibilidade     legibilidade     1   \n",
       "mesma            mesma            1   \n",
       "objetivo         objetivo         1   \n",
       "obrigatóriamente obrigatóriamente 1   \n",
       "output           output           1   \n",
       "outputs          outputs          1   \n",
       "possuem          possuem          1   \n",
       "resulta          resulta          1   \n",
       "técnicas         técnicas         1   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>word</th><th scope=col>freq</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>que</th><td>que        </td><td>10         </td></tr>\n",
       "\t<tr><th scope=row>palavra</th><td>palavra    </td><td> 9         </td></tr>\n",
       "\t<tr><th scope=row>uma</th><td>uma        </td><td> 8         </td></tr>\n",
       "\t<tr><th scope=row>são</th><td>são        </td><td> 8         </td></tr>\n",
       "\t<tr><th scope=row>lema</th><td>lema       </td><td> 8         </td></tr>\n",
       "\t<tr><th scope=row>não</th><td>não        </td><td> 7         </td></tr>\n",
       "\t<tr><th scope=row>mesmo</th><td>mesmo      </td><td> 6         </td></tr>\n",
       "\t<tr><th scope=row>lematização</th><td>lematização</td><td> 6         </td></tr>\n",
       "\t<tr><th scope=row>palavras</th><td>palavras   </td><td> 6         </td></tr>\n",
       "\t<tr><th scope=row>exemplo</th><td>exemplo    </td><td> 5         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & word & freq\\\\\n",
       "\\hline\n",
       "\tque & que         & 10         \\\\\n",
       "\tpalavra & palavra     &  9         \\\\\n",
       "\tuma & uma         &  8         \\\\\n",
       "\tsão & são         &  8         \\\\\n",
       "\tlema & lema        &  8         \\\\\n",
       "\tnão & não         &  7         \\\\\n",
       "\tmesmo & mesmo       &  6         \\\\\n",
       "\tlematização & lematização &  6         \\\\\n",
       "\tpalavras & palavras    &  6         \\\\\n",
       "\texemplo & exemplo     &  5         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | word | freq |\n",
       "|---|---|---|\n",
       "| que | que         | 10          |\n",
       "| palavra | palavra     |  9          |\n",
       "| uma | uma         |  8          |\n",
       "| são | são         |  8          |\n",
       "| lema | lema        |  8          |\n",
       "| não | não         |  7          |\n",
       "| mesmo | mesmo       |  6          |\n",
       "| lematização | lematização |  6          |\n",
       "| palavras | palavras    |  6          |\n",
       "| exemplo | exemplo     |  5          |\n",
       "\n"
      ],
      "text/plain": [
       "            word        freq\n",
       "que         que         10  \n",
       "palavra     palavra      9  \n",
       "uma         uma          8  \n",
       "são         são          8  \n",
       "lema        lema         8  \n",
       "não         não          7  \n",
       "mesmo       mesmo        6  \n",
       "lematização lematização  6  \n",
       "palavras    palavras     6  \n",
       "exemplo     exemplo      5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'casmurro'</li>\n",
       "\t<li>'certeza'</li>\n",
       "\t<li>'dizem'</li>\n",
       "\t<li>'dom'</li>\n",
       "\t<li>'mas'</li>\n",
       "\t<li>'não'</li>\n",
       "\t<li>'por'</li>\n",
       "\t<li>'que'</li>\n",
       "\t<li>'também'</li>\n",
       "\t<li>'tenho'</li>\n",
       "\t<li>'mesmo'</li>\n",
       "\t<li>'bem'</li>\n",
       "\t<li>'com'</li>\n",
       "\t<li>'conseguiu'</li>\n",
       "\t<li>'seu'</li>\n",
       "\t<li>'tentou'</li>\n",
       "\t<li>'ter'</li>\n",
       "\t<li>'uma'</li>\n",
       "\t<li>'stemming'</li>\n",
       "\t<li>'exemplo'</li>\n",
       "\t<li>'formas'</li>\n",
       "\t<li>'palavra'</li>\n",
       "\t<li>'para'</li>\n",
       "\t<li>'são'</li>\n",
       "\t<li>'fonte'</li>\n",
       "\t<li>'como'</li>\n",
       "\t<li>'gata'</li>\n",
       "\t<li>'gatas'</li>\n",
       "\t<li>'gato'</li>\n",
       "\t<li>'gatos'</li>\n",
       "\t<li>'lema'</li>\n",
       "\t<li>'lematização'</li>\n",
       "\t<li>'palavras'</li>\n",
       "\t<li>'processo'</li>\n",
       "\t<li>'stemização'</li>\n",
       "\t<li>'igualmente'</li>\n",
       "\t<li>'infinitivo'</li>\n",
       "\t<li>'tem'</li>\n",
       "\t<li>'tinha'</li>\n",
       "\t<li>'tiver'</li>\n",
       "\t<li>'todas'</li>\n",
       "\t<li>'bom'</li>\n",
       "\t<li>'lexemas'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'casmurro'\n",
       "\\item 'certeza'\n",
       "\\item 'dizem'\n",
       "\\item 'dom'\n",
       "\\item 'mas'\n",
       "\\item 'não'\n",
       "\\item 'por'\n",
       "\\item 'que'\n",
       "\\item 'também'\n",
       "\\item 'tenho'\n",
       "\\item 'mesmo'\n",
       "\\item 'bem'\n",
       "\\item 'com'\n",
       "\\item 'conseguiu'\n",
       "\\item 'seu'\n",
       "\\item 'tentou'\n",
       "\\item 'ter'\n",
       "\\item 'uma'\n",
       "\\item 'stemming'\n",
       "\\item 'exemplo'\n",
       "\\item 'formas'\n",
       "\\item 'palavra'\n",
       "\\item 'para'\n",
       "\\item 'são'\n",
       "\\item 'fonte'\n",
       "\\item 'como'\n",
       "\\item 'gata'\n",
       "\\item 'gatas'\n",
       "\\item 'gato'\n",
       "\\item 'gatos'\n",
       "\\item 'lema'\n",
       "\\item 'lematização'\n",
       "\\item 'palavras'\n",
       "\\item 'processo'\n",
       "\\item 'stemização'\n",
       "\\item 'igualmente'\n",
       "\\item 'infinitivo'\n",
       "\\item 'tem'\n",
       "\\item 'tinha'\n",
       "\\item 'tiver'\n",
       "\\item 'todas'\n",
       "\\item 'bom'\n",
       "\\item 'lexemas'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'casmurro'\n",
       "2. 'certeza'\n",
       "3. 'dizem'\n",
       "4. 'dom'\n",
       "5. 'mas'\n",
       "6. 'não'\n",
       "7. 'por'\n",
       "8. 'que'\n",
       "9. 'também'\n",
       "10. 'tenho'\n",
       "11. 'mesmo'\n",
       "12. 'bem'\n",
       "13. 'com'\n",
       "14. 'conseguiu'\n",
       "15. 'seu'\n",
       "16. 'tentou'\n",
       "17. 'ter'\n",
       "18. 'uma'\n",
       "19. 'stemming'\n",
       "20. 'exemplo'\n",
       "21. 'formas'\n",
       "22. 'palavra'\n",
       "23. 'para'\n",
       "24. 'são'\n",
       "25. 'fonte'\n",
       "26. 'como'\n",
       "27. 'gata'\n",
       "28. 'gatas'\n",
       "29. 'gato'\n",
       "30. 'gatos'\n",
       "31. 'lema'\n",
       "32. 'lematização'\n",
       "33. 'palavras'\n",
       "34. 'processo'\n",
       "35. 'stemização'\n",
       "36. 'igualmente'\n",
       "37. 'infinitivo'\n",
       "38. 'tem'\n",
       "39. 'tinha'\n",
       "40. 'tiver'\n",
       "41. 'todas'\n",
       "42. 'bom'\n",
       "43. 'lexemas'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"casmurro\"    \"certeza\"     \"dizem\"       \"dom\"         \"mas\"        \n",
       " [6] \"não\"         \"por\"         \"que\"         \"também\"      \"tenho\"      \n",
       "[11] \"mesmo\"       \"bem\"         \"com\"         \"conseguiu\"   \"seu\"        \n",
       "[16] \"tentou\"      \"ter\"         \"uma\"         \"stemming\"    \"exemplo\"    \n",
       "[21] \"formas\"      \"palavra\"     \"para\"        \"são\"         \"fonte\"      \n",
       "[26] \"como\"        \"gata\"        \"gatas\"       \"gato\"        \"gatos\"      \n",
       "[31] \"lema\"        \"lematização\" \"palavras\"    \"processo\"    \"stemização\" \n",
       "[36] \"igualmente\"  \"infinitivo\"  \"tem\"         \"tinha\"       \"tiver\"      \n",
       "[41] \"todas\"       \"bom\"         \"lexemas\"    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creating a dataframe\n",
    "matriz <- data.frame(word=names(matriz), freq = matriz)\n",
    "matriz\n",
    "\n",
    "head(matriz, n=10)\n",
    "\n",
    "findFreqTerms(TextDoc_matrix, lowfreq = 2, highfreq = Inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a30cdd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>$dom</strong> = <dl class=dl-horizontal>\n",
       "\t<dt>casmurro</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>mas</dt>\n",
       "\t\t<dd>0.74</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\textbf{\\$dom} = \\begin{description*}\n",
       "\\item[casmurro] 1\n",
       "\\item[mas] 0.74\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "**$dom** = casmurro\n",
       ":   1mas\n",
       ":   0.74\n",
       "\n"
      ],
      "text/plain": [
       "$dom\n",
       "casmurro      mas \n",
       "    1.00     0.74 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>$dom</strong> = <dl class=dl-horizontal>\n",
       "\t<dt>casmurro</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>mas</dt>\n",
       "\t\t<dd>0.74</dd>\n",
       "\t<dt>não</dt>\n",
       "\t\t<dd>0.54</dd>\n",
       "\t<dt>amor</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>aí</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>brilham</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>certeza</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>dizem</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>feliz</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>fica</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>isso</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>mais</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>meu</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>meus</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>olhos</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>quando</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>sim</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>sorriso</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>também</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>vejo</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>antigas</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>das</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>isto</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>memórias</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>passar</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>repassar</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>saudade</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>amar…</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>aqui</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>até</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>bem</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>conseguiu</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>continuar</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>epitáfio</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>escrito</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>estava</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>está</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>expectativas</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>frustradas</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>letra</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>lá</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>nessa</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>pequena</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>pois</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>possuiu</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>prosseguiu</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>ser</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>tentou</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "\t<dt>vida</dt>\n",
       "\t\t<dd>0.53</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\textbf{\\$dom} = \\begin{description*}\n",
       "\\item[casmurro] 1\n",
       "\\item[mas] 0.74\n",
       "\\item[não] 0.54\n",
       "\\item[amor] 0.53\n",
       "\\item[aí] 0.53\n",
       "\\item[brilham] 0.53\n",
       "\\item[certeza] 0.53\n",
       "\\item[dizem] 0.53\n",
       "\\item[feliz] 0.53\n",
       "\\item[fica] 0.53\n",
       "\\item[isso] 0.53\n",
       "\\item[mais] 0.53\n",
       "\\item[meu] 0.53\n",
       "\\item[meus] 0.53\n",
       "\\item[olhos] 0.53\n",
       "\\item[quando] 0.53\n",
       "\\item[sim] 0.53\n",
       "\\item[sorriso] 0.53\n",
       "\\item[também] 0.53\n",
       "\\item[vejo] 0.53\n",
       "\\item[antigas] 0.53\n",
       "\\item[das] 0.53\n",
       "\\item[isto] 0.53\n",
       "\\item[memórias] 0.53\n",
       "\\item[passar] 0.53\n",
       "\\item[repassar] 0.53\n",
       "\\item[saudade] 0.53\n",
       "\\item[amar…] 0.53\n",
       "\\item[aqui] 0.53\n",
       "\\item[até] 0.53\n",
       "\\item[bem] 0.53\n",
       "\\item[conseguiu] 0.53\n",
       "\\item[continuar] 0.53\n",
       "\\item[epitáfio] 0.53\n",
       "\\item[escrito] 0.53\n",
       "\\item[estava] 0.53\n",
       "\\item[está] 0.53\n",
       "\\item[expectativas] 0.53\n",
       "\\item[frustradas] 0.53\n",
       "\\item[letra] 0.53\n",
       "\\item[lá] 0.53\n",
       "\\item[nessa] 0.53\n",
       "\\item[pequena] 0.53\n",
       "\\item[pois] 0.53\n",
       "\\item[possuiu] 0.53\n",
       "\\item[prosseguiu] 0.53\n",
       "\\item[ser] 0.53\n",
       "\\item[tentou] 0.53\n",
       "\\item[vida] 0.53\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "**$dom** = casmurro\n",
       ":   1mas\n",
       ":   0.74não\n",
       ":   0.54amor\n",
       ":   0.53aí\n",
       ":   0.53brilham\n",
       ":   0.53certeza\n",
       ":   0.53dizem\n",
       ":   0.53feliz\n",
       ":   0.53fica\n",
       ":   0.53isso\n",
       ":   0.53mais\n",
       ":   0.53meu\n",
       ":   0.53meus\n",
       ":   0.53olhos\n",
       ":   0.53quando\n",
       ":   0.53sim\n",
       ":   0.53sorriso\n",
       ":   0.53também\n",
       ":   0.53vejo\n",
       ":   0.53antigas\n",
       ":   0.53das\n",
       ":   0.53isto\n",
       ":   0.53memórias\n",
       ":   0.53passar\n",
       ":   0.53repassar\n",
       ":   0.53saudade\n",
       ":   0.53amar…\n",
       ":   0.53aqui\n",
       ":   0.53até\n",
       ":   0.53bem\n",
       ":   0.53conseguiu\n",
       ":   0.53continuar\n",
       ":   0.53epitáfio\n",
       ":   0.53escrito\n",
       ":   0.53estava\n",
       ":   0.53está\n",
       ":   0.53expectativas\n",
       ":   0.53frustradas\n",
       ":   0.53letra\n",
       ":   0.53lá\n",
       ":   0.53nessa\n",
       ":   0.53pequena\n",
       ":   0.53pois\n",
       ":   0.53possuiu\n",
       ":   0.53prosseguiu\n",
       ":   0.53ser\n",
       ":   0.53tentou\n",
       ":   0.53vida\n",
       ":   0.53\n",
       "\n"
      ],
      "text/plain": [
       "$dom\n",
       "    casmurro          mas          não         amor           aí      brilham \n",
       "        1.00         0.74         0.54         0.53         0.53         0.53 \n",
       "     certeza        dizem        feliz         fica         isso         mais \n",
       "        0.53         0.53         0.53         0.53         0.53         0.53 \n",
       "         meu         meus        olhos       quando          sim      sorriso \n",
       "        0.53         0.53         0.53         0.53         0.53         0.53 \n",
       "      também         vejo      antigas          das         isto     memórias \n",
       "        0.53         0.53         0.53         0.53         0.53         0.53 \n",
       "      passar     repassar      saudade        amar…         aqui          até \n",
       "        0.53         0.53         0.53         0.53         0.53         0.53 \n",
       "         bem    conseguiu    continuar     epitáfio      escrito       estava \n",
       "        0.53         0.53         0.53         0.53         0.53         0.53 \n",
       "        está expectativas   frustradas        letra           lá        nessa \n",
       "        0.53         0.53         0.53         0.53         0.53         0.53 \n",
       "     pequena         pois      possuiu   prosseguiu          ser       tentou \n",
       "        0.53         0.53         0.53         0.53         0.53         0.53 \n",
       "        vida \n",
       "        0.53 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find words that are correlated with \"dom\" with a coefficient > .70\n",
    "findAssocs(TextDoc_matrix, \"dom\", .70)\n",
    "\n",
    "# Find words that are correlated with \"dom\" with a coefficient > .50\n",
    "findAssocs(TextDoc_matrix, \"dom\", .50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "08cbd397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<DocumentTermMatrix (documents: 14, terms: 168)>>\n",
      "Non-/sparse entries: 242/2110\n",
      "Sparsity           : 90%\n",
      "Maximal term length: 17\n",
      "Weighting          : term frequency (tf)\n",
      "Sample             :\n",
      "    Terms\n",
      "Docs exemplo lema lematização mesmo não palavra palavras que são uma\n",
      "  1        0    0           0     0   1       0        0   3   0   0\n",
      "  10       1    4           1     2   0       1        2   0   3   1\n",
      "  12       0    0           1     0   0       0        0   0   0   0\n",
      "  13       0    0           1     1   2       4        0   0   1   3\n",
      "  2        0    0           0     1   0       0        0   0   0   0\n",
      "  3        0    0           0     0   4       0        0   0   0   1\n",
      "  4        0    0           0     0   0       0        0   1   0   0\n",
      "  5        1    0           0     0   0       1        0   3   1   2\n",
      "  7        0    1           1     0   0       3        1   1   0   1\n",
      "  8        1    3           1     2   0       0        3   1   3   0\n"
     ]
    }
   ],
   "source": [
    "# Matrix creation term-document\n",
    "#TextDoc_matrix <- TermDocumentMatrix(TextDoc)\n",
    "\n",
    "Doc_TermMatriz <- DocumentTermMatrix(TextDoc) \n",
    "inspect(Doc_TermMatriz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8d74f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
